---
title: "Classification"
author: "Ryan, David"
date: "2/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(ISLR)
```

# Classification Chapter 4 (pg 127)

For use when the response variable is qualitative. linear regression does not work. Example, three categories of outcome (stroke, drug overdose, epileptic seizure) and linear regression assumes there is a progression or 'order' (mild, moderate, severe) with equal difference in the outcomes. In reality, there is no order and the differences are not equal (unknown, just a different category). 
Exception is if the outcome is binary, then a linear regression could fit. Example, if Yhat is < or > 0.5 an outcome is chosen. Note that some predictions may be outside the 0 to 1 interval! This happens whenever a straight line (linear regression) is applied to a binary response. 

## Logistic Regression (pg 130)
This provides an output between 0-1 for all values of probability of X or p(X). 

Logistic Function:
p(x) = eB0+B1X // 1+eB0+B1X

the output of this function gives use the odds:
p(x) // 1-p(x)

This is the same odds ratio of a horse race bet 1:1, 9:1, 1/4:1. Answers the question "What are the odds of *categorical outcome* happening?"
We can also get the log-odds or logit: 

log(p(X)//1-p(X) = B0 +B1X

This is very similar to a linear equation; however, a unit increase in X does not increase the outcome by B1. This is a key difference. Logistical regression a increase by one unit of X changes the log odds or logit (above) by B1. Note the relation between probability of X or p(X) and X is not linear (this is not a linear model). The change in p(X) is directly related to the value of X. 

## Estimating the Regression Coefficients (PG 133)
Previously in linear regression we used the least squares methods to estimate the coefficients, this function uses Maximum Likelihood (can also be used in linear regression). 

In the example we see that balance (B1 = 0.0055), this means that a one unit increase in balance increases the PROBABILITY of default by 0.0055. To examine how accuracy of the coefficient by getting the z-statistic
z = B1//SE(B1)
A large value is evidence against the null (B1 = 0). 
The intercept is not of interest, it's main purpose is to adjust the average fitted probabilities to the proportion of the ones in the data. 

## Making Predictions (pg 134)
What is E???
```{r}
E = ?? 
B0=-10.6513
B1 = 0.0055
BalanceX = 1000
(B0 + (B1*BalanceX)) / (1+B0 + (B1*BalanceX))
```
## Multiple Logistic Regression
Adding more predictors is good, can find confounding variables and better explain effects. Example from book shows that Student 0|1 by run by itself is a positive coefficient, being a student increases the chance of defaulting on loan. However, when other predictors of balance and income are added, Student is not a negative coefficient, being a student decreases chances of defaulting on loan. How did this happen? When averaging out other predictors and not taking them into account (running Student by itself), Students on average carry a higher balance and higher balance is correlated with higher chances of default. However, when Student and Balance are included we see that there is a separation that shows students have a higher balance but have a lower default rate. Both need to be included to see this separation. 

## Logistic regression for > 2 Classes
This is possible for Logistical Regression, however, Linear Discriminant Analysis is preferred 

## Linear Discriminant Analysis (138)
Better than Logistic for multiple classes - if classes are well separated logistic is unstable or if small n. 



Use the credit card Default data set.   

```{r}
Default = Default
plot(Default$balance, Default$income, pch = c("+", "*"), col = c('blue', 'red'))
```


