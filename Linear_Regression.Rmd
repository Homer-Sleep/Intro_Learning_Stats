---
title: "Linear Regression"
author: "Ryan, David"
date: "12/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Linear Regression

Labs

```{r}
library(MASS)
library(ISLR)
# Linear Regression
?Boston
lm.fit1 = lm(medv~lstat, data = Boston)
lm.fit1
summary(lm.fit1)
coef(lm.fit1)
confint(lm.fit1) # give confidence interval for coefficient estimates
# predict() can produce confidence intervals for the prediction of x for a given value of y
predict (lm.fit1, data.frame(lstat = c(5,10,15)), interval = "confidence")
# and prediction intervals 
predict (lm.fit1, data.frame(lstat = c(5,10,15)), interval = "prediction")
# each are centered around the coefficient point, wider 95% intervals for prediction 
# plot with least squares regression line with abline()
with(Boston, plot(lstat,medv))
abline(lm.fit1)
# abline() does just that, provides a line with intercept and slop (a,b). This is the output when we call the lm.fit1. We can also plot any line, just input an intercept and slope for (a,b)
# lwd = # increases the width of the line by a factor #
# pch = # changes the symbol, here are all the symbols 1:20
with(Boston, plot(1:20, 1:20, pch = 1:20)) 

with(Boston, plot(lstat,medv))
abline(lm.fit1, lwd = 3, col = "red")
with(Boston, plot(lstat,medv, col = "red"))
with(Boston, plot(lstat,medv, pch = 20))
with(Boston, plot(lstat,medv, pch = "+"))

# plot(lm()) will produce 4 output panels individually
plot(lm.fit1)
# use par() splits into panels to show in one figure, here a 2x2 grid
par(mfrow = c(2,2))
plot(lm.fit1)

# Alternatively, we can compute the residuals with the residuals(). The rstudent() will return studentized residuals. Theses are used to plot the residuals vs. the fitted. 
plot(predict(lm.fit1), residuals(lm.fit1)) #
plot(predict(lm.fit1), rstudent(lm.fit1))

# On the basis of residual plots, there could be some non-linearity. leverage (outlier??) statistics can be computed for any number of predictors using the hatvalues()

plot(hatvalues(lm.fit1))
which.max(hatvalues(lm.fit1))
```

## 
Multiple Regression
##
If there are multiple predictors, a simple regression ignores these when coefficients are estimated. This is of great importance if there is a correlation between predictors, an interaction. 
Multiple regression allows for each predictor to have its own slope (B1, B2, ... Bp). To estimate multiple coefficients matrix algebra is used (not covered here). 
Comparing the output shows the effect of each predictor while holding the other predictors constant, or controlling for other predictors.

Important Questions:

1) Is there a relation between the response and the predictor? 
simple linear regression null hypothesis - is predictor (B) =0? Multiple regression null - are any of the predictors (Bj) non-zero? 











