---
title: "Linear Regression"
author: "Ryan, David"
date: "12/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Linear Regression

Labs

```{r}
library(MASS)
library(ISLR)
# Linear Regression
?Boston
lm.fit1 = lm(medv~lstat, data = Boston)
lm.fit1
summary(lm.fit1)
coef(lm.fit1)
confint(lm.fit1) # give confidence interval for coefficient estimates
# predict() can produce confidence intervals for the prediction of x for a given value of y
predict (lm.fit1, data.frame(lstat = c(5,10,15)), interval = "confidence")
# and prediction intervals 
predict (lm.fit1, data.frame(lstat = c(5,10,15)), interval = "prediction")
# each are centered around the coefficient point, wider 95% intervals for prediction 
# plot with least squares regression line with abline()
with(Boston, plot(lstat,medv))
abline(lm.fit1)
# abline() does just that, provides a line with intercept and slop (a,b). This is the output when we call the lm.fit1. We can also plot any line, just input an intercept and slope for (a,b)
# lwd = # increases the width of the line by a factor #
# pch = # changes the symbol, here are all the symbols 1:20
with(Boston, plot(1:20, 1:20, pch = 1:20)) 

with(Boston, plot(lstat,medv))
abline(lm.fit1, lwd = 3, col = "red")
with(Boston, plot(lstat,medv, col = "red"))
with(Boston, plot(lstat,medv, pch = 20))
with(Boston, plot(lstat,medv, pch = "+"))

# plot(lm()) will produce 4 output panels individually
plot(lm.fit1)
# use par() splits into panels to show in one figure, here a 2x2 grid
par(mfrow = c(2,2))
plot(lm.fit1)

# Alternatively, we can compute the residuals with the residuals(). The rstudent() will return studentized residuals. Theses are used to plot the residuals vs. the fitted. 
plot(predict(lm.fit1), residuals(lm.fit1)) #
plot(predict(lm.fit1), rstudent(lm.fit1))

# On the basis of residual plots, there could be some non-linearity. leverage (outlier??) statistics can be computed for any number of predictors using the hatvalues()

plot(hatvalues(lm.fit1))
which.max(hatvalues(lm.fit1))
```

## 
Multiple Regression
##
If there are multiple predictors, a simple regression ignores these when coefficients are estimated. This is of great importance if there is a correlation between predictors, an interaction. 
Multiple regression allows for each predictor to have its own slope (B1, B2, ... Bp). To estimate multiple coefficients matrix algebra is used (not covered here). 
Comparing the output shows the effect of each predictor while holding the other predictors constant, or controlling for other predictors.

Important Questions (pg 75):

1) Is there a relation between the response and the predictor? 
simple linear regression null hypothesis - is predictor (B) =0? Multiple regression null - are any of the predictors (Bj) non-zero? 
This is related to an *F* test - *F* = (Total SS - Residual SS)/ num of predictors(p) // Residual SS / (n - p - 1)
Numerators shows the SS accounted for by each predictor and the denominator shows the residual variance divided by the sample size with a predictor penalty. 

Partial effect - examines the effect of adding a variable to a model
To examine the partial effect of certain variables (the number of which is called *q*), one model includes all variables (RSS) and another has the certain variables excluded (RSS0). Apply these to an *F* test - *F* = (RSS0 - RSS) / *q* // RSS/ (n-p-1). 
Numerator shows the difference of residual variance between the two models for each predictor examined (*q*). 
This type of *F* test that omits a predictor is the same as the *t*(^2) test provided by the model, this shows the partial effect of adding each predictor. 

Using the *t* and p-value for each predictor is flawed as it does not correct for the number of predictors in the model (like the *F* test does), for which there is a 5% chance of a type II error. 

When *p* is > n, we need forward selection or other high-dimensional settings, chapter 6. 

2) Deciding on important variables
Looking at the p-values for each predictor is one way to assess the important ones; however, when *p* is high there could be a type II error. A process called variable selection is a better approach. (Chap. 6)
The method mentioned above becomes problematic with multiple predictors. It takes the comparison of 4 models to evaluate 2 predictors. It takes 2^*p* to evaluate in this manner. 

Other methods include:

Forward Selection: start with a null model and fit *p* simple linear models. Then add to the null the predictor that has the lowest RSS. Then add the next predictor that results in the lowest RSS until a stopping threshold is met. 

Backward selection: start with all predictors in the model and then remove the predictor with the largest p-value. Refit the new model and repeat till a threshold it met (like all variables have p< 0.001. 

Mixed Method: Starts with the forward approach of adding the predictor with the best fit. As predictors or other variables are added the p-values change. If the p-value of a variable goes above a threshold then it is removed. These steps go back and forth until all variables in the model are below a threshold and the ones out of the model are above a threshold. 

Backward selection cannot be used when *p* > n. Mixed approach address the greediness or redundant variables of the forward approach. 

3) Model Fit:
RSE (residual SE) and R^2. R^2 is the square of the correlation of the response and predictor. For multiple regression it is the correlation of the (Y and Yhat)^2. Note, a fitted linear model finds the maximum correlation between the predicted outcome variable and the outcome variable. The R^2 is a range 0-1. As we add more predictors to the model we get higher R^2; however, this may be a result of overfitting when the increase of the R^2 is minimal. 
Similar to R^2, RSE can be used to examine model fit for each predictor added to the model. If adding a predictor only decreases the RSE slightly or increases RSE, then it is not useful in the model. 

RSE = sqrt of 1/ n-p-1 * RSS

So RSE does have a penalty of adding predictors while R^2 does not, even though RSS decreases with more predictors. This balances the trade off of adding predictors and how much variance they account for. 

You can also plot the data to show the variance of each data point to the regression line. If there is an interaction of predictors, the true fit might be non-linear. 

4) Predictions (pg 82)
Once we have a model it is easy to make predictions; however there are three types of uncertainty to address: 
1) the coefficient estimates (B0, B1, B2) are estimates of the true population. inaccuracy of the coefficient estimates is related to the reducible error (can be addressed with model shape/approach). Confidence interval shows how close the Y(hat) is to the true *f*(X). 

2) Part of the reducible error is introduced by the model bias. The true f(X) may not be linear even though we are using a linear model. 

3) Even if we knew f(X), that is the real values of Bp, the response cannot be predicted perfectly due to the error in the model (irreducible error). *Prediction Intervals* are used to estimate how much true Y will vary from Y(hat). Prediction intervals are wider than confidence intervals because it incorporates the reducible error and the irreducible error. 

Confidence interval looks at all data points, or average of the outcome variable, and gives a 95% CI range. The prediction interval looks a the prediction CI of a singular outcome variable, not the average ov all outcome variables. The prediction CI range is much wider given the increased uncertainty of a singular prediction accuray vs. the predicted accuracy of the average of the outcome variable. 

Multiple Regression Lab (pg 113)

```{r}
# lm() is used for multiple regression with the syntax of lm(y ~ x1 + x2 + x3)
lm.fit1 = lm(medv~lstat+age, data = Boston)
summary(lm.fit1)

# to include all variables in a data set as predictors use "." :
lm.fit1 = lm(medv ~ ., data = Boston)
summary(lm.fit1)

# want to look at specific parts of the model output?
?summary.lm

# R^2
summary(lm.fit1)$r.sq
# RSE
summary(lm.fit1)$sigma

# car package can calculate the variance inflation factors (VIF) <- WTF is that ??
library(car)
vif(lm.fit1)
# what if we are using the Backward selection method and want all predictors except those that are not p< 0.5? this removes age 
lm.fit2 = lm(medv ~ . -age, data = Boston)
summary(lm.fit2)

#can also update() models
lm.fit2 = update(lm.fit1, ~.-age)
summary(lm.fit2)


```

Interaction Terms
```{r}
# syntax lstat:black is the explicit way of including an interaction term. Easier method is lstat*black, this included the two predictors of lstat and black as well as the interaction term. 
summary(lm(medv ~ lstat*age, data = Boston))
```

**Other Consideration in the Regression Model** (pg 83)

Qualitative predictors:
Qualitative or factors are usually handled as a dummy variable. 
First example: Code as 1 = female and 0 = male. This makes B0 the average effect for male and B1 is the difference of female verses male. 
2nd: 1 = female, -1 = male. B0 is the average result ignoring male/female effects. B0+B1 would be female and B0- B1 would be male, thus B1 would equal the average difference between M/F. 

More than two levels:
B1 = 0 Not Asian, B1 = 1 is Asian. 
B2 = 0 Not Caucasian, B2 = 1 is Caucasian. 
B0 = is African American. This is the baseline. B1 and B2 apply the difference of each group from the baseline group. 
Remember, you can regress Y on whatever qualitative and quantitative predictors you want. 

**Extensions to the Linear Model** (pg 87)

Linear Regression has two important assumptions about the relation between predictor and outcome:
1) the shape of the relation is linear
2) and Additive:
  Additive assumption is that the effect from a predictor on the response is independent of other predictors. That the change in the response Y due to a one-unit change in X is constant, regardless of the value of X. 
  
How to address additive assumption:
This assumption can not correctly model an interaction effect (e.g., one predictor interacts (changes the effect) of another predictor). 
Introducing a third (interaction) term allows for two predictors to have a combined effect on the outcome. 
Y = B0+ B1(X1) + B2(X2) + B3(X1 x X2) + e
or
Y = β0 + (β1 + β3X2)X1 + β2X2 + e 
Y = β0 + β˜ β1X1 + β2X2 + e
β˜ changes with X2, the effect of X1 on Y is no longer constant, that is changing X2 will alter the effect of X1 on Y. 



1.2 + 3.4 × lines + 0.22 × workers + 1.4 × (lines × workers)
= 1.2 + (3.4 + 1.4 × workers) × lines + 0.22 × workers.

```{r}
lines = 4
workers = 20

1.2 + 3.4 * lines  + 1.4 * (lines * workers)+ 0.22 * workers
1.2 + (3.4 + 1.4 * workers) * lines + 0.22 * workers # this just adds the coefficients of lines and works together then multiplies them by the number of lines and workers. 
```
The interaction term (B3) captures the change in the effectiveness of one variable for one unit increase in another variable. Listed on the output table the of TV x Radio would mean the amount of change in TV effectiveness given one unit of Radio.
```{r}
# the model without the interaction has R2 = .897 and with interaction term R2 = .968 so...
M1_R2 = 89.7
M2_R2 = 96.8
(M2_R2 - M1_R2) / (100 - M1_R2) # numer is the difference in the models (7.1% higher with interaction) and denom (10.3 %) is the variance unexplained by the main effects only model. 
# this means that 69% of the variability in sales (outcome) that remains after fitting the additive model (main effects only) has been explained by the interaction term.
```


If there are no main effects but a significant interaction, the main effects should stay in the model. It does not hurt to keep them and the main effects are usually correlated with the interaction so removing them can alter the interpretation of the model. 

Qualitative variable interactions work well. They create a separate line for each group: 
outcome = B1+ (B0+B2 {for student} or B0 {for non-student})

Student and non-student have different intercepts (B0 + B2 or B0) but the same slope (B1). Now add the grouping interaction term:
outcome  = (B0 + B2) + (B1 + B3) for student and B0 + B1 for non student

This allows for the interaction of the predictors thus, allows for different slopes (B1+B3) vs. (B1). 



Non-linear Relations:
Extend the linear model with polynomials (e.g., ^1:5). 


**Potential Problems** (pg 92)
These problems can occur when fitting a linear model:

1) Non-linearity of the response-predictor relation
2) correlation of error terms
3) Non-constant variance of error terms
4) Outliers
5) High-leverage points
6) Collinearity

addressing these can be an art applied science. 
1) Non-linearity of the Data
Linear regression assumes a strait line relation between predictor and outcome. If there is a non-linear relation, then residual plots can help visualize the pattern. Residual plots take e = y- y(hat) on the y-axis and the predictor on the x-axis. Deviation from the middle of the y-axis shows the difference of the predicted vs. actual outcome. Across the x-axis patterns can emerge that show how the real data are diffent from the model prediction. Adjustments to the model can be based on the residual plot. 







































